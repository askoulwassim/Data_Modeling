# Data Warehouse in AWS
Data Warehouse For Sparkify hosted on RedShift in AWS (Music Liberary Startup)

## Use Case of This Project

Sparkify is a music streaming app with directory of JSON logs on user activity on the app, and JSON metadata on songs in their app. The objective is to build an ETL pipeline that extracts their data from S3, stages them in Redshift, and transforms data into a set of dimensional tables for their analytics team to continue finding insights in what songs their users are listening to. We decided that we need a key distribution style copying of data for an optimized performance as the team scales. We chose to use Python to write up the database schema and ETL pipeline using a Postgres database.

Based on our need for a simple querying of the data for the analytics team, we decided to elect a Star schema of our fact and deminsion tables. Our focus in this database is to gain fast aggregated insights tailored for specific needs of the analytics team, a star schema allows us to build an OLAP system around our data.

## Database Setup

We have two datasets that we are working with: song dataset & user logs dataset. The song dataset files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset.

```
song_data/A/B/C/TRABCEI128F424C983.json
song_data/A/A/B/TRAABJL12903CDCF1A.json
```

And below is an example of what a single song file, TRAABJL12903CDCF1A.json, looks like.
    
``` JSON
{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}
```

The user logs dataset simulate activity logs from a music streaming app based on specified configurations. The log files in the dataset are partitioned by year and month. For example, here are filepaths to two files in this dataset.
    
```
log_data/2018/11/2018-11-12-events.json
log_data/2018/11/2018-11-13-events.json
``` 

Based on the datasets mentioned and that we need to optimize for queries on song play analysis, we need our fact table to be on songs played with every data point on every instance a song is played. While our dimension tables would be divided to users, songs, artists, and time. Here is the following table structures:

###### Fact Table

```
1. songplays - records in log data associated with song plays i.e. records with page NextSong
```
*songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent*

###### Dimension Tables

```
2. users - users in the app
```
*user_id, first_name, last_name, gender, level*

```
3. songs - songs in music database
```
*song_id, title, artist_id, year, duration*

```
4. artists - artists in music database
```
*artist_id, name, location, latitude, longitude*

```
5. time - timestamps of records in songplays broken down into specific units
```
*start_time, hour, day, week, month, year, weekday*

## Using The Project

1.Check links of the data provided in the S3 section of the dwh.cfg file. The song dataset is a subsection of sets from [The Million Song Dataset](http://millionsongdataset.com/), and the user log dataset is generated by this [event simulator](https://github.com/Interana/eventsim).

2. Insert the relevant information in the dwh.cfg file to connect to the Amazon RedShift IAM user already setup in AWS. Values needed for the cluster are HOST, DB_NAME, DB_USER, DB_PASSWORD, and DB_PORT *Please note that the port number is usually 5439.* Next is the IAM role ARN link, e.g.: 'https://XXXXXXXXXXXX.signin.aws.amazon.com/console'

3. Run the create_tables.py in order to create the database with RedShift in AWS.

*Note: You need to make sure that Pandas and JSON liberaries are installed.*

4. Run etl.py which will go through 71 song files and 30 user log files importing the data into the database using the sql_queries.py.

*Check sql_queries.py for the quesries used to create the database.*

Congratulations! You moved the data of a fake music streaming app to the cloud. You can apply for a position with Spotify now! Again! :grin: