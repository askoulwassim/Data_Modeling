# Data Lake in AWS using Spark
Data Lake For Sparkify using Spark in AWS (Music Liberary Startup)

## Use Case of This Project

Sparkify is a music streaming app with directory of JSON logs on user activity on the app, and JSON metadata on songs in their app. The objective is to build an ETL pipeline that extracts their data from S3, stages them in Redshift, and transforms data into a set of dimensional tables for their analytics team to continue finding insights in what songs their users are listening to. We decided that we need a key distribution style copying of data for an optimized performance as the team scales. We chose to use Python to write up the database schema and ETL pipeline using a Postgres database.

Based on our need for a simple querying of the data for the analytics team, we decided to elect a Star schema of our fact and deminsion tables. Our focus in this database is to gain fast aggregated insights tailored for specific needs of the analytics team, a star schema allows us to build an OLAP system around our data.

## Database Setup

We have two datasets that we are working with: song dataset & user logs dataset. The song dataset files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset.

```
song_data/A/B/C/TRABCEI128F424C983.json
song_data/A/A/B/TRAABJL12903CDCF1A.json
```

And below is an example of what a single song file, TRAABJL12903CDCF1A.json, looks like.
    
``` JSON
{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}
```

The user logs dataset simulate activity logs from a music streaming app based on specified configurations. The log files in the dataset are partitioned by year and month. For example, here are filepaths to two files in this dataset.
    
```
log_data/2018/11/2018-11-12-events.json
log_data/2018/11/2018-11-13-events.json
``` 

Based on the datasets mentioned and that we need to load data in row form for potentially later use cases of the data, we need to create a data lake for our raw dtaa files. While our dimension tables would be divided to users, songs, artists, and time. Here is the following table structures:

###### Fact Table

```
1. songplays - records in log data associated with song plays i.e. records with page NextSong
```
*songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent*

###### Dimension Tables

```
2. users - users in the app
```
*user_id, first_name, last_name, gender, level*

```
3. songs - songs in music database
```
*song_id, title, artist_id, year, duration*

```
4. artists - artists in music database
```
*artist_id, name, location, latitude, longitude*

```
5. time - timestamps of records in songplays broken down into specific units
```
*start_time, hour, day, week, month, year, weekday*

## Using The Project

1.Check links of the data provided in the S3 section of the dwh.cfg file. The song dataset is a subsection of sets from [The Million Song Dataset](http://millionsongdataset.com/), and the user log dataset is generated by this [event simulator](https://github.com/Interana/eventsim).

2. Insert the relevant information in the dl.cfg file to connect to your account on AWS IAM user already setup. Values needed for the cluster are Access_Key_ID, and Access_Secret_Key.

3. Run the etl.py in order to create the data lake with Spark and load the tables to an S3 bucket on AWS.

*Note: You need to make sure that Pandas and JSON liberaries are installed.*

Congratulations! You have created a data lake for the raw data of a fake music streaming app to AWS. You can apply for a position with Spotify now! Another one! :grin: