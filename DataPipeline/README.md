# Data Pipeline in AWS using Airflow
Data Pipeline For Sparkify using Apache Airflow in AWS (Music Liberary Startup)

## Use Case of This Project

Sparkify is a music streaming app with directory of JSON logs on user activity on the app, and JSON metadata on songs in their app. The objective is to build to build an Airflow pipeline is to add an additional layer of automation and monitoring to the ETL processes that feed into the Sparkify Data warehouse. The implemented Airflow pipeline reads user session and song data from S3, loads the data into staging, and then populates dimension and facts tables in RedShift. We decided that we need a key distribution style copying of data for an optimized performance as the team scales. We chose to use Python to write up the database schema and ETL pipeline using a Postgres database.

## Database Setup

We have two datasets that we are working with: song dataset & user logs dataset. The song dataset files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset.

```
song_data/A/B/C/TRABCEI128F424C983.json
song_data/A/A/B/TRAABJL12903CDCF1A.json
```

And below is an example of what a single song file, TRAABJL12903CDCF1A.json, looks like.
    
``` JSON
{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}
```

The user logs dataset simulate activity logs from a music streaming app based on specified configurations. The log files in the dataset are partitioned by year and month. For example, here are filepaths to two files in this dataset.
    
```
log_data/2018/11/2018-11-12-events.json
log_data/2018/11/2018-11-13-events.json
``` 

Based on the datasets mentioned and that we need to load data in row form for potentially later use cases of the data, we need to create a data lake for our raw dtaa files. While our dimension tables would be divided to users, songs, artists, and time. Here is the following table structures:

###### Fact Table

```
1. songplays - records in log data associated with song plays i.e. records with page NextSong
```
*songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent*

###### Dimension Tables

```
2. users - users in the app
```
*user_id, first_name, last_name, gender, level*

```
3. songs - songs in music database
```
*song_id, title, artist_id, year, duration*

```
4. artists - artists in music database
```
*artist_id, name, location, latitude, longitude*

```
5. time - timestamps of records in songplays broken down into specific units
```
*start_time, hour, day, week, month, year, weekday*

2. AWS credentials and Redshift Connections must be configured in Airflow

## Elements of The Project

The following python scripts were used to develop this pipeline:

1. dp_dag.py: Defines the dag and assigns operators to each respective task in the Airflow pipeline. Additionally, it dictates the sequence of tasks at the bottom of the file.

2. sql_queries.py: Defines SQL queries to read and load staging, fact, and dim tables. Found in plugins/helpers directory

3. stage_redshift.py: Defines StageToRedshiftOperator that reads from S3 and writes to staging tables in AWS Redshfit

4. load_fact.py: Defines LoadFactOperator that extracts data from staging into songplays (or otherwise defined) fact table

5. load_dimension.py: Defines LoadDimensionOperator that extracts data from staging into the song, user, artist, and time dim tables

6. data_quality.py: Defines DataQualityOperator that counts the rows in each table to ensure data has been written to RedShift

## Using The Project

1. Check links of the data provided in the S3 section of the dwh.cfg file. The song dataset is a subsection of sets from [The Million Song Dataset](http://millionsongdataset.com/), and the user log dataset is generated by this [event simulator](https://github.com/Interana/eventsim).

2. Run create_tables.sql file in order to create the staging, dimension, and fact tables in RedShift.

3. Configure your AWS credentials and RedShift connections in Airflow, check [here](https://towardsdatascience.com/how-to-deploy-airflow-on-aws-best-practices-63778d6eab9e) for more info on the topic.

Note: You need to make sure that Pandas and JSON liberaries are installed.*

Congratulations! You have created a data lake for the raw data of a fake music streaming app to AWS. You can apply for a position with Spotify now! Another one! :grin: